{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col, count, when, max\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Make sure project root is in Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration for spark instance\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceBehavior\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.hadoop.hadoop.native.lib\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-19 03:35:46|      view|  30200005|2053013554449088861|                NULL|   elari|  77.2|512412397|f62be3c5-18af-4ab...|\n",
      "|2019-11-26 09:16:08|      view|   1005115|2053013555631882655|electronics.smart...|   apple|916.37|568675496|c857db53-cd0a-480...|\n",
      "|2019-11-10 12:50:50|      view|  15700275|2053013559733912211|                NULL|imperial|206.16|513262731|c637d18a-6fc5-4c1...|\n",
      "|2019-11-04 09:23:52|      view|   1004589|2053013555631882655|electronics.smart...|    inoi| 61.36|562973725|e41d3c3f-830e-48d...|\n",
      "|2019-11-29 12:11:17|  purchase|   5300157|2053013563173241677|                NULL| philips| 37.56|560750791|0538a90a-6395-413...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = spark.read.option(\"header\", True).csv(\"../data/scaledData-2019-Nov.csv\")\n",
    "\n",
    "# Convert timestamp column\n",
    "df = df.withColumn(\"event_time\", to_timestamp(\"event_time\"))\n",
    "\n",
    "# Preview\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-19 03:35:46|      view|  30200005|2053013554449088861|                NULL|   elari|  77.2|512412397|f62be3c5-18af-4ab...|\n",
      "|2019-11-26 09:16:08|      view|   1005115|2053013555631882655|electronics.smart...|   apple|916.37|568675496|c857db53-cd0a-480...|\n",
      "|2019-11-10 12:50:50|      view|  15700275|2053013559733912211|                NULL|imperial|206.16|513262731|c637d18a-6fc5-4c1...|\n",
      "|2019-11-04 09:23:52|      view|   1004589|2053013555631882655|electronics.smart...|    inoi| 61.36|562973725|e41d3c3f-830e-48d...|\n",
      "|2019-11-29 12:11:17|  purchase|   5300157|2053013563173241677|                NULL| philips| 37.56|560750791|0538a90a-6395-413...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'price' column to float\n",
    "df = df.withColumn(\"price\", col(\"price\").cast(\"float\"))\n",
    "\n",
    "# Drop rows with missing essential columns\n",
    "df = df.dropna(subset=[\"user_id\", \"user_session\"])\n",
    "\n",
    "# Filter relevant events (view, cart, purchase)\n",
    "df = df.filter(df.event_type.isin([\"view\", \"cart\", \"purchase\"]))\n",
    "\n",
    "# Show the cleaned data preview\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Views': 1929582, 'Add to Cart (cart)': 249718, 'Purchase': 89603, 'View to Cart Drop-off': 1679864, 'Cart to Purchase Drop-off': 160115}\n"
     ]
    }
   ],
   "source": [
    "# Funnel analysis with corrected event type\n",
    "view_df = df.filter(df.event_type == \"view\").select(\"user_id\").distinct()\n",
    "cart_df = df.filter(df.event_type == \"cart\").select(\"user_id\").distinct()\n",
    "purchase_df = df.filter(df.event_type == \"purchase\").select(\"user_id\").distinct()\n",
    "\n",
    "view_count = view_df.count()\n",
    "cart_count = cart_df.count()\n",
    "purchase_count = purchase_df.count()\n",
    "\n",
    "funnel_data = {\n",
    "    \"Views\": view_count,\n",
    "    \"Add to Cart (cart)\": cart_count,\n",
    "    \"Purchase\": purchase_count,\n",
    "    \"View to Cart Drop-off\": view_count - cart_count,\n",
    "    \"Cart to Purchase Drop-off\": cart_count - purchase_count\n",
    "}\n",
    "\n",
    "print(funnel_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------+-----+\n",
      "|        user_session|view_count|cart_count|purchase_count|label|\n",
      "+--------------------+----------+----------+--------------+-----+\n",
      "|879b893f-feb2-43d...|         2|         0|             0|    0|\n",
      "|11ccc7df-5d85-499...|         4|         0|             0|    0|\n",
      "|f2ab18d2-5759-402...|         1|         0|             0|    0|\n",
      "|ee8dd117-fa84-47d...|         1|         0|             0|    0|\n",
      "|077035b3-376b-48b...|         1|         0|             0|    0|\n",
      "+--------------------+----------+----------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot like event counting using conditional aggregation\n",
    "session_df = df.groupBy(\"user_session\").agg(\n",
    "    count(when(col(\"event_type\") == \"view\", True)).alias(\"view_count\"),\n",
    "    count(when(col(\"event_type\") == \"cart\", True)).alias(\"cart_count\"),\n",
    "    count(when(col(\"event_type\") == \"purchase\", True)).alias(\"purchase_count\")\n",
    ")\n",
    "\n",
    "# Create label: 1 if purchase_count > 0, else 0\n",
    "session_df = session_df.withColumn(\"label\", when(col(\"purchase_count\") > 0, 1).otherwise(0))\n",
    "\n",
    "session_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7438\n"
     ]
    }
   ],
   "source": [
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"view_count\", \"cart_count\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled_df = assembler.transform(session_df)\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Predict on test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.7745\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Predict and evaluate\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training model 1/14: rf_50_5_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_50_5_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_50_5_32_feature_importances.csv\n",
      "AUC for rf_50_5_32: 0.7745\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_50_5_32_metadata.json\n",
      "\n",
      "🚀 Training model 2/14: rf_100_5_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_100_5_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_100_5_32_feature_importances.csv\n",
      "AUC for rf_100_5_32: 0.7745\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_100_5_32_metadata.json\n",
      "\n",
      "🚀 Training model 3/14: rf_200_5_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_200_5_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_200_5_32_feature_importances.csv\n",
      "AUC for rf_200_5_32: 0.8123\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_200_5_32_metadata.json\n",
      "\n",
      "🚀 Training model 4/14: rf_50_10_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_50_10_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_50_10_32_feature_importances.csv\n",
      "AUC for rf_50_10_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_50_10_32_metadata.json\n",
      "\n",
      "🚀 Training model 5/14: rf_100_10_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_100_10_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_100_10_32_feature_importances.csv\n",
      "AUC for rf_100_10_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_100_10_32_metadata.json\n",
      "\n",
      "🚀 Training model 6/14: rf_200_10_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_200_10_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_200_10_32_feature_importances.csv\n",
      "AUC for rf_200_10_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_200_10_32_metadata.json\n",
      "\n",
      "🚀 Training model 7/14: rf_50_20_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_50_20_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_50_20_32_feature_importances.csv\n",
      "AUC for rf_50_20_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_50_20_32_metadata.json\n",
      "\n",
      "🚀 Training model 8/14: rf_100_20_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_100_20_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_100_20_32_feature_importances.csv\n",
      "AUC for rf_100_20_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_100_20_32_metadata.json\n",
      "\n",
      "🚀 Training model 9/14: rf_200_20_32\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_200_20_32_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_200_20_32_feature_importances.csv\n",
      "AUC for rf_200_20_32: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_200_20_32_metadata.json\n",
      "\n",
      "🚀 Training model 10/14: rf_50_5_64\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_50_5_64_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_50_5_64_feature_importances.csv\n",
      "AUC for rf_50_5_64: 0.7745\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_50_5_64_metadata.json\n",
      "\n",
      "🚀 Training model 11/14: rf_100_5_64\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_100_5_64_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_100_5_64_feature_importances.csv\n",
      "AUC for rf_100_5_64: 0.7745\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_100_5_64_metadata.json\n",
      "\n",
      "🚀 Training model 12/14: rf_200_5_64\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_200_5_64_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_200_5_64_feature_importances.csv\n",
      "AUC for rf_200_5_64: 0.8123\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_200_5_64_metadata.json\n",
      "\n",
      "🚀 Training model 13/14: rf_100_10_64\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_100_10_64_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_100_10_64_feature_importances.csv\n",
      "AUC for rf_100_10_64: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_100_10_64_metadata.json\n",
      "\n",
      "🚀 Training model 14/14: rf_200_20_64\n",
      "✅ Predictions saved: D:/ecomm-bigdata-project/output\\predictions\\rf_200_20_64_predictions.csv\n",
      "✅ Feature importances saved: D:/ecomm-bigdata-project/output\\feature_importances\\rf_200_20_64_feature_importances.csv\n",
      "AUC for rf_200_20_64: 0.8126\n",
      "✅ Metadata saved: D:/ecomm-bigdata-project/output\\model_metadata\\rf_200_20_64_metadata.json\n",
      "\n",
      "🎯 All model AUC results saved to: D:/ecomm-bigdata-project/output\\all_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Paths\n",
    "base_output_dir = \"D:/ecomm-bigdata-project/output\"\n",
    "predictions_dir = os.path.join(base_output_dir, \"predictions\")\n",
    "feature_dir = os.path.join(base_output_dir, \"feature_importances\")\n",
    "metadata_dir = os.path.join(base_output_dir, \"model_metadata\")\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = [\n",
    "    (50, 5, 32),\n",
    "    (100, 5, 32),\n",
    "    (200, 5, 32),\n",
    "    (50, 10, 32),\n",
    "    (100, 10, 32),\n",
    "    (200, 10, 32),\n",
    "    (50, 20, 32),\n",
    "    (100, 20, 32),\n",
    "    (200, 20, 32),\n",
    "    (50, 5, 64),\n",
    "    (100, 5, 64),\n",
    "    (200, 5, 64),\n",
    "    (100, 10, 64),\n",
    "    (200, 20, 64),\n",
    "]\n",
    "\n",
    "# Store all model results\n",
    "results = []\n",
    "\n",
    "for i, (numTrees, maxDepth, maxBins) in enumerate(param_grid):\n",
    "    model_name = f\"rf_{numTrees}_{maxDepth}_{maxBins}\"\n",
    "    print(f\"\\n🚀 Training model {i+1}/{len(param_grid)}: {model_name}\")\n",
    "\n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",\n",
    "                                 numTrees=numTrees, maxDepth=maxDepth, maxBins=maxBins)\n",
    "    model = rf.fit(train_df)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Save predictions\n",
    "    pred_path = os.path.join(predictions_dir, f\"{model_name}_predictions.csv\")\n",
    "    predictions.select(\"user_session\", \"prediction\", \"probability\").toPandas().to_csv(pred_path, index=False)\n",
    "    print(f\"✅ Predictions saved: {pred_path}\")\n",
    "\n",
    "    # Save feature importances\n",
    "    fi = model.featureImportances.toArray()\n",
    "    fi_df = pd.DataFrame(fi, columns=[\"importance\"])\n",
    "    fi_path = os.path.join(feature_dir, f\"{model_name}_feature_importances.csv\")\n",
    "    fi_df.to_csv(fi_path, index=False)\n",
    "    print(f\"✅ Feature importances saved: {fi_path}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"AUC for {model_name}: {auc:.4f}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"numTrees\": numTrees,\n",
    "        \"maxDepth\": maxDepth,\n",
    "        \"maxBins\": maxBins,\n",
    "        \"AUC\": auc\n",
    "    }\n",
    "    meta_path = os.path.join(metadata_dir, f\"{model_name}_metadata.json\")\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"✅ Metadata saved: {meta_path}\")\n",
    "\n",
    "    # Store result for summary\n",
    "    results.append((model_name, auc))\n",
    "\n",
    "# Save all model results in one CSV\n",
    "df_results = pd.DataFrame(results, columns=[\"model_name\", \"auc\"])\n",
    "results_path = os.path.join(base_output_dir, \"all_model_results.csv\")\n",
    "df_results.to_csv(results_path, index=False)\n",
    "print(f\"\\n🎯 All model AUC results saved to: {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Best model: rf_50_20_32 with AUC: 0.8126\n"
     ]
    }
   ],
   "source": [
    "from src.model import pick_best_model\n",
    "\n",
    "best_model_name, best_auc = pick_best_model(\"D:/ecomm-bigdata-project/output/all_model_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1038a5e847933c0019329b0e0a849b51d1b8d0e8e35084f65728a6e48b58d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
