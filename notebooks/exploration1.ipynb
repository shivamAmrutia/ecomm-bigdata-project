{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup sys.path to import from src/\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Force Spark to bind and advertise on localhost\n",
    "#    (or \"host.docker.internal\" if you're in Docker on Windows)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "# 2) Build your SparkSession with matching driver configs\n",
    "spark = (\n",
    "    SparkSession\n",
    "      .builder\n",
    "      .appName(\"LoadAndPredict\")\n",
    "      .master(\"local[1]\")\n",
    "      .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "      .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "      # If youâ€™re inside Windows Docker you may need:\n",
    "      # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "      .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Import custom functions\n",
    "from src.preprocessing import load_data, clean_data, feature_engineer, assemble_features\n",
    "from src.save_outputs import save_predictions, save_feature_importances, save_model_metadata\n",
    "from src.model import pick_best_model_from_grid, evaluate_model, manual_grid_search_rf, manual_grid_search_dt, manual_grid_search_gbt, manual_grid_search_lr, manual_grid_search_nb, train_and_log_mlflow\n",
    "from src.mongo_export import create_mongo_database, get_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ecommerce Behavior Exploration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-19 08:35:...|      view|  30200005|2053013554449088861|                NULL|   elari|  77.2|512412397|f62be3c5-18af-4ab...|\n",
      "|2019-11-26 14:16:...|      view|   1005115|2053013555631882655|electronics.smart...|   apple|916.37|568675496|c857db53-cd0a-480...|\n",
      "|2019-11-10 17:50:...|      view|  15700275|2053013559733912211|                NULL|imperial|206.16|513262731|c637d18a-6fc5-4c1...|\n",
      "|2019-11-04 14:23:...|      view|   1004589|2053013555631882655|electronics.smart...|    inoi| 61.36|562973725|e41d3c3f-830e-48d...|\n",
      "|2019-11-29 17:11:...|  purchase|   5300157|2053013563173241677|                NULL| philips| 37.56|560750791|0538a90a-6395-413...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw dataset\n",
    "file_path = \"../data/testData-2019-Nov.csv\"\n",
    "raw_df = load_data(file_path, spark)\n",
    "\n",
    "# Quick look at raw data\n",
    "raw_df.show(5)\n",
    "raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "|2019-11-19 03:35:46|      view|  30200005|2053013554449088861|                NULL|   elari|  77.2|512412397|f62be3c5-18af-4ab...|\n",
      "|2019-11-26 09:16:08|      view|   1005115|2053013555631882655|electronics.smart...|   apple|916.37|568675496|c857db53-cd0a-480...|\n",
      "|2019-11-10 12:50:50|      view|  15700275|2053013559733912211|                NULL|imperial|206.16|513262731|c637d18a-6fc5-4c1...|\n",
      "|2019-11-04 09:23:52|      view|   1004589|2053013555631882655|electronics.smart...|    inoi| 61.36|562973725|e41d3c3f-830e-48d...|\n",
      "|2019-11-29 12:11:17|  purchase|   5300157|2053013563173241677|                NULL| philips| 37.56|560750791|0538a90a-6395-413...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the dataset\n",
    "clean_df = clean_data(raw_df)\n",
    "\n",
    "# Quick check after cleaning\n",
    "clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+-------------+-------------------+-------------------+---------+----------------+\n",
      "|        user_session|num_views|num_cart_adds|num_purchases|      session_start|        session_end|avg_price|session_duration|\n",
      "+--------------------+---------+-------------+-------------+-------------------+-------------------+---------+----------------+\n",
      "|08327d25-5fe5-4ec...|        1|            0|            0|2019-11-10 23:43:37|2019-11-10 23:43:37|     39.9|               0|\n",
      "|cbd7e3a0-2c5e-493...|        1|            0|            0|2019-11-17 15:48:48|2019-11-17 15:48:48|     4.61|               0|\n",
      "|a08f39b4-71f4-431...|        1|            0|            0|2019-11-24 10:21:36|2019-11-24 10:21:36|    23.81|               0|\n",
      "|544365e8-d7e6-4ea...|        1|            1|            0|2019-11-09 11:44:13|2019-11-09 11:46:48|    47.47|             155|\n",
      "|71697745-4b20-419...|        1|            0|            0|2019-11-17 12:22:24|2019-11-17 12:22:24|   1083.4|               0|\n",
      "+--------------------+---------+-------------+-------------+-------------------+-------------------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new features if needed\n",
    "feature_df = feature_engineer(clean_df)\n",
    "\n",
    "# Quick preview\n",
    "feature_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clean_df.groupBy(\"user_session\").agg(\n",
    "    (F.max(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0))).alias(\"label\")\n",
    ")\n",
    "\n",
    "final_df = feature_df.join(labels, on=\"user_session\", how=\"left\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns you want to use\n",
    "feature_cols = ['num_views', 'num_cart_adds', 'session_duration', 'avg_price']\n",
    "\n",
    "# Assemble features\n",
    "final_df = assemble_features(final_df, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:00:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\amrut\\AppData\\Local\\Temp\\tmptjx14ikc\\model, flavor: spark). Fall back to return ['pyspark==3.5.5']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/05/04 13:00:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Logged run with AUC=0.9366\n",
      "ðŸƒ View run defiant-ram-266 at: http://localhost:5000/#/experiments/1/runs/7fdafa6c452c47789d0cbd81be42165e\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "train_and_log_mlflow(train_df, test_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:09:55 INFO mlflow.spark: URI 'runs:/7fdafa6c452c47789d0cbd81be42165e/spark-model/sparkml' does not point to the current DFS.\n",
      "2025/05/04 13:09:55 INFO mlflow.spark: File 'runs:/7fdafa6c452c47789d0cbd81be42165e/spark-model/sparkml' not found on DFS. Will attempt to upload the file.\n",
      "2025/05/04 13:09:55 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/543ee1c3-0316-4d3e-8045-3d152a9db10b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from run: 7fdafa6c452c47789d0cbd81be42165e\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 4) Construct the model URI and load it\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/spark-model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 5) Now you can call .transform() on any DataFrame with the same schema\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# e.g. reuse your `test_df` from before, or create a new one:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mtransform(test_df)\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlflow\\spark\\__init__.py:985\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_uri, dfs_tmpdir, dst_path)\u001b[0m\n\u001b[0;32m    983\u001b[0m sparkml_model_uri \u001b[39m=\u001b[39m append_to_uri_path(model_uri, flavor_conf[\u001b[39m\"\u001b[39m\u001b[39mmodel_data\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    984\u001b[0m local_sparkml_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(local_mlflow_model_path, flavor_conf[\u001b[39m\"\u001b[39m\u001b[39mmodel_data\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m _load_model(\n\u001b[0;32m    986\u001b[0m     model_uri\u001b[39m=\u001b[39;49msparkml_model_uri,\n\u001b[0;32m    987\u001b[0m     dfs_tmpdir_base\u001b[39m=\u001b[39;49mdfs_tmpdir,\n\u001b[0;32m    988\u001b[0m     local_model_path\u001b[39m=\u001b[39;49mlocal_sparkml_model_path,\n\u001b[0;32m    989\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlflow\\spark\\__init__.py:909\u001b[0m, in \u001b[0;36m_load_model\u001b[1;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_model_databricks_dbfs(\n\u001b[0;32m    906\u001b[0m         dfs_tmpdir, local_model_path \u001b[39mor\u001b[39;00m _download_artifact_from_uri(model_uri)\n\u001b[0;32m    907\u001b[0m     )\n\u001b[0;32m    908\u001b[0m model_uri \u001b[39m=\u001b[39m _HadoopFileSystem\u001b[39m.\u001b[39mmaybe_copy_from_uri(model_uri, dfs_tmpdir, local_model_path)\n\u001b[1;32m--> 909\u001b[0m \u001b[39mreturn\u001b[39;00m PipelineModel\u001b[39m.\u001b[39;49mload(model_uri)\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39;49mload(path)\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPipelineModel\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[39m=\u001b[39m DefaultParamsReader\u001b[39m.\u001b[39;49mloadMetadata(path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msc)\n\u001b[0;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m metadata[\u001b[39m\"\u001b[39m\u001b[39mparamMap\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m metadata[\u001b[39m\"\u001b[39m\u001b[39mparamMap\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPython\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[39mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[39m\"\u001b[39m\u001b[39mJavaMLReadable[PipelineModel]\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls))\u001b[39m.\u001b[39mload(path)\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mtextFile(metadataPath, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mfirst()\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[39m=\u001b[39m DefaultParamsReader\u001b[39m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[39mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   2889\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   2857\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   2511\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\amrut\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# 1) Point at your MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# 2) (Optional) pick your experiment by name\n",
    "experiment_name = \"test_lr_experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# 3) Find the most recent run ID in that experiment\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "# look up the experiment\n",
    "exp = client.get_experiment_by_name(experiment_name)\n",
    "if exp is None:\n",
    "    raise ValueError(f\"Experiment '{experiment_name}' not found\")\n",
    "# fetch runs, ordered by start time descending\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    order_by=[\"attribute.start_time DESC\"],\n",
    "    max_results=1,\n",
    ")\n",
    "if not runs:\n",
    "    raise ValueError(f\"No runs found in experiment '{experiment_name}'\")\n",
    "run_id = runs[0].info.run_id\n",
    "print(\"Loading model from run:\", run_id)\n",
    "\n",
    "# 4) Construct the model URI and load it\n",
    "model_uri = f\"runs:/{run_id}/spark-model\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "# 5) Now you can call .transform() on any DataFrame with the same schema\n",
    "# e.g. reuse your `test_df` from before, or create a new one:\n",
    "predictions = loaded_model.transform(test_df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grid_search_rf(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grid_search_lr(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grid_search_dt(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_grid_search_nb(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grid_search_gbt(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest model\n",
    "\n",
    "base_rf_dir = \"../output/rf/\"\n",
    "base_lr_dir = \"../output/lr/\"\n",
    "base_dt_dir = \"../output/dt/\"\n",
    "base_nb_dir = \"../output/nb/\"\n",
    "base_gbt_dir = \"../output/gbt/\"\n",
    "\n",
    "dic = {\"model\":\"\", \"params\": \"\", \"AUC\": 0 }\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"RF: \")\n",
    "results_csv_path_rf = os.path.join(base_rf_dir, \"grid_search_results.csv\")\n",
    "results.append(pick_best_model_from_grid(results_csv_path_rf, base_rf_dir).split(','))\n",
    "\n",
    "print(\"LR: \")\n",
    "results_csv_path_lr = os.path.join(base_lr_dir, \"grid_search_results.csv\")\n",
    "results.append(pick_best_model_from_grid(results_csv_path_lr, base_lr_dir).split(','))\n",
    "\n",
    "print(\"DT: \")\n",
    "results_csv_path_dt = os.path.join(base_dt_dir, \"grid_search_results.csv\")\n",
    "results.append(pick_best_model_from_grid(results_csv_path_dt, base_dt_dir).split(','))\n",
    "\n",
    "print(\"NB: \")\n",
    "results_csv_path_nb = os.path.join(base_nb_dir, \"grid_search_results.csv\")\n",
    "results.append(pick_best_model_from_grid(results_csv_path_nb, base_nb_dir).split(','))\n",
    "\n",
    "print(\"GBT: \")\n",
    "results_csv_path_gbt = os.path.join(base_gbt_dir, \"grid_search_results.csv\")\n",
    "results.append(pick_best_model_from_grid(results_csv_path_gbt, base_gbt_dir).split(','))\n",
    "\n",
    "for r in results:\n",
    "    if float(r[1]) >= dic[\"AUC\"]:\n",
    "        dic[\"model\"] = r[2]\n",
    "        dic[\"params\"] = r[0][:-2]\n",
    "        dic[\"AUC\"] = float(r[1])\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a list of best hyperparameters or feature names\n",
    "import json\n",
    "\n",
    "with open(\"../output/best_params.json\", \"w\") as f:\n",
    "    json.dump(dic, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions CSV (adjust path if needed)\n",
    "model, params = dic[\"model\"], dic[\"params\"]\n",
    "predictions_path = f\"../output/{model}/{model}_{params}/predictions.csv\"\n",
    "df_predictions = pd.read_csv(predictions_path)\n",
    "\n",
    "df_predictions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mongo_database(df_predictions.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "output_dir = \"../output/figures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "collection = get_collection()\n",
    "# Fetch from MongoDB\n",
    "df_mongo = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "# Drop MongoDB's default _id field\n",
    "df_mongo = df_mongo.drop(columns=[\"_id\"], errors='ignore')\n",
    "\n",
    "# --- Visualization 1: Conversion Funnel ---\n",
    "funnel = {\n",
    "    \"Viewed\": df_mongo.shape[0],\n",
    "    \"Cart Add\": df_mongo[df_mongo[\"num_cart_adds\"] > 0].shape[0],\n",
    "    \"Purchased\": df_mongo[df_mongo[\"label\"] == 1].shape[0]\n",
    "}\n",
    "sns.barplot(x=list(funnel.keys()), y=list(funnel.values()))\n",
    "plt.title(\"User Conversion Funnel\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"conversion_funnel.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 2: Predicted Purchase Probabilities ---\n",
    "if 'probability' in df_mongo.columns:\n",
    "    sns.histplot(df_mongo['probability'], bins=30, kde=True)\n",
    "    plt.title(\"Predicted Purchase Probability\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"purchase_probability.png\"), dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_df = pd.DataFrame({\n",
    "    \"Stage\": [\"Viewed\", \"Cart Add\", \"Purchased\"],\n",
    "    \"Users\": [funnel[\"Viewed\"], funnel[\"Cart Add\"], funnel[\"Purchased\"]]\n",
    "})\n",
    "funnel_df[\"Dropoff %\"] = 100 * (1 - funnel_df[\"Users\"] / funnel_df[\"Users\"].iloc[0])\n",
    "\n",
    "sns.lineplot(data=funnel_df, x=\"Stage\", y=\"Dropoff %\", marker=\"o\")\n",
    "plt.title(\"Drop-Off Rate at Funnel Stages\")\n",
    "plt.ylabel(\"Drop-Off (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/dropoff_rate.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_mongo, x=\"label\", y=\"session_duration\")\n",
    "plt.xticks([0, 1], [\"No Purchase\", \"Purchase\"])\n",
    "plt.title(\"Session Duration vs Purchase Behavior\")\n",
    "plt.ylabel(\"Duration (s)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/session_duration_vs_purchase.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_mongo, x=\"avg_price\", hue=\"label\", bins=40, kde=True, palette=\"Set2\")\n",
    "plt.title(\"Price Distribution by Purchase Label\")\n",
    "plt.xlabel(\"Average Price of Viewed Products\")\n",
    "plt.ylabel(\"Session Count\")\n",
    "plt.legend([\"No Purchase\", \"Purchase\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/price_vs_purchase.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1038a5e847933c0019329b0e0a849b51d1b8d0e8e35084f65728a6e48b58d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
